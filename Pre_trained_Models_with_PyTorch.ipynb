{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frankeldios/cyber-surfer/blob/main/Pre_trained_Models_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8abd2f3a-7930-44bc-9a5f-3646677309bd"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \">\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0b80864-134b-4e19-b978-042c3752181d"
      },
      "source": [
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e5bdfa8-6014-495e-af4c-1905a13cf122"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53741054-55b8-4d99-9976-6032dbb90087"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions:\n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li>\n",
        "<li>  identify  several  misclassified samples</li>\n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3c08cf-34b0-406d-8125-0593277f34bc"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "921f07a2-6a2d-4608-84f2-0cc62bf2501b"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ceb7083-02ed-4c9c-b5b4-7b0418388a2a"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efedf9be-c643-4d62-8158-0918061c6b8b"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f119a703-4c0b-40c8-9ca9-152a26a98210",
        "outputId": "5a058f71-fff8-4bf5-e956-43387993a61e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-02 13:50:46--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  28.3MB/s    in 91s     \n",
            "\n",
            "2024-04-02 13:52:17 (27.3 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e3f2804b-7bc0-4a34-a8bd-0756372003da"
      },
      "outputs": [],
      "source": [
        "!unzip -q Positive_tensors.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5119dc8-afc5-460d-879a-8b774f567bd0",
        "outputId": "f59425aa-a710-45e4-c228-c8e41aeab621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-02 13:54:40--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  12.2MB/s    in 3m 2s   \n",
            "\n",
            "2024-04-02 13:57:42 (11.1 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fad15709-e387-40fd-ab2f-8fde44dea3e1"
      },
      "source": [
        "We will install torchvision:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a4397a6-b3f6-4b0e-b9f9-c0e294eede06",
        "outputId": "79a6f818-f28a-4a4f-f93d-5a7d03dfca9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.2.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "720b2e1a-fa06-4daf-a922-4a70777f6709"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cadbf87-12b4-4cf5-973b-d074375b21f7"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "100c4913-0f97-425c-bf42-eba819ed5f7f",
        "outputId": "bdea64de-9e16-4d82-abf2-96d8038530cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe922073bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "62927ada-7de8-485c-a08e-cb2b038b25d6"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fed9c29-48b2-4bbf-9ba9-7f6fc1c088a2"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7b81ceb-2ff9-4e71-b0ad-bcd507f91029"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8630dc80-3ee1-40a4-84d7-0427cd7101c7"
      },
      "source": [
        " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c2612bc-5ed4-4f7d-bc9d-71c6a69ce2b7",
        "outputId": "ffff2bd9-b315-4ade-fe6e-df36f70e2632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/content\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files\n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "\n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)\n",
        "\n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "\n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y, idx\n",
        "\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "50RI1u8MEoak",
        "outputId": "fbef02d6-e53c-424c-dcda-7590ee117212"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "747173bb-89d3-45e8-b058-ab209f14610c"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0618234d-d2a4-459a-aed0-20e3803a4661",
        "outputId": "c0d02c59-e107-4420-a2a9-b39d81a7e38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d03d6186-c5e3-4594-b469-fc776d407fe5"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67c3bc6f-c9ce-4bc6-98e2-160b4c2c6be3"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cdd3ebc-0de2-4418-9316-a20a12ec7034"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "293cde0f-d36f-4584-a1ff-d4fe736b9fb0",
        "outputId": "4034fb5c-ae5d-4f1d-c16b-d248f9dd5822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 86.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "\n",
        "model = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89b310a4-2eb5-4627-ae5e-d0783ba838ad"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "22ed14f3-ded5-47a6-b667-34e9d5bc0b95"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03f23176-eca4-4e8f-9ec2-a164a5a7ef65"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "410287ff-6594-4af8-8acc-495106d31545"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4f79a8c7-4e3c-48b2-8d5c-75ec66fc7b88"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(512, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "048fe114-92ee-4c41-aede-1e016711ffcd"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1462f12b-da03-4175-ad74-043e46166410",
        "outputId": "8a125066-6990-4aa9-9204-08e1955a2233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb183bcf-8cfa-4e48-93e8-af78f42e57b0"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91768582-592a-4360-b47c-1c7db7008ff8"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8455f1a9-a0af-4502-9179-0a4693cf06d8"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5263c76f-483d-42bf-9716-c526278d3fe5"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the loss function\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a14f9645-a2ff-4900-91e7-4acf3eec2427"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f006c789-b1d6-4eb9-bdc4-613265ac440e"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a965344-294c-4f35-881b-6f3b7e938149"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4ffbf141-4354-429f-ba64-cf0fecf4d97e"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "278f8e4c-8cc9-477a-b291-3aedf0d0852e"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a7f9e3b-f4a4-430d-92e4-2b204f4f9162"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "e10db4f0-56f4-4c94-940f-133f5764ef04"
      },
      "outputs": [],
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    loss_sublist = []\n",
        "    for x, y, idx in train_loader:\n",
        "\n",
        "        model.train()\n",
        "        #clear gradient\n",
        "        optimizer.zero_grad()\n",
        "        #make a prediction\n",
        "        z=model(x)\n",
        "        # calculate loss\n",
        "        loss=criterion(z,y)\n",
        "        loss_sublist.append(loss.data.item())\n",
        "        # calculate gradients of parameters\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        loss_list.append(np.mean(loss_sublist))\n",
        "\n",
        "    correct=0\n",
        "    for x_test, y_test, idx in validation_loader:\n",
        "        # set model to eval\n",
        "        model.eval()\n",
        "        #make a prediction\n",
        "        z = model(x_test)\n",
        "        #find max\n",
        "        _,yhat = torch.max(z.data, 1)\n",
        "\n",
        "        #Calculate misclassified  samples in mini-batch\n",
        "        #hint +=(yhat==y_test).sum().item()\n",
        "        correct +=(yhat==y_test).sum().item()\n",
        "\n",
        "    accuracy=correct/N_test\n",
        "    accuracy_list.append(accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "176f3003-c65d-40bc-96ad-5c9c48c99f3b"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f321eee5-544b-4659-839f-0e6ea591d09d",
        "outputId": "2b6221c9-db86-4482-e37c-7f34df4e182a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9934"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "3c7ae1d7-abbd-4e21-b0f2-9e45b967a1b6",
        "outputId": "44bd983c-03f9-4f16-a50c-9ce56f22cb8c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7f1f6bc-f2ba-4b06-9109-7778966e1379"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a78f947-6f88-4871-8005-d5732cd8e2d9"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6d0864db-4423-447e-b379-407e707efb43"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUsUMdMkEoao",
        "outputId": "d757d02b-0a9b-46b8-cdfe-806fcdcf3515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 17 yhat: tensor([1]) y:tensor([0])\n",
            "sample 118 yhat: tensor([0]) y:tensor([1])\n",
            "sample 120 yhat: tensor([0]) y:tensor([1])\n",
            "sample 228 yhat: tensor([0]) y:tensor([1])\n"
          ]
        }
      ],
      "source": [
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1)\n",
        "\n",
        "# Plot the misclassified samples\n",
        "count = 0\n",
        "for (x_test, y_test, idx) in validation_loader:\n",
        "    z = model(x_test.reshape(-1,3,224,224))\n",
        "    _,yhat = torch.max(z.data, 1)\n",
        "    if yhat != y_test:\n",
        "        print(\"sample {} yhat: {} y:{}\".format(tf.reshape(idx, []), yhat, y_test))\n",
        "        count += 1\n",
        "    if count >= 4:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "715b8fe6-26bd-4bb9-b8da-1ca492528ee6"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46f20a3f-7d1b-4aea-9e74-e373ec30e1bb"
      },
      "source": [
        "<h2>About the Authors:</h2>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb64cce-3fe5-489d-bced-79c3e7a447cf"
      },
      "source": [
        "\n",
        "## Change Log\n",
        "\n",
        "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
        "|---|---|---|---|\n",
        "| 2020-09-21  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a379170-e56f-40f9-9f8f-e3227416419a"
      },
      "source": [
        "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">MIT License</a>.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}